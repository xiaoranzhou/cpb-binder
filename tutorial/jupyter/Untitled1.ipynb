{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('gihub_orgs.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"names\"\n",
    "    start_urls = [\n",
    "        'https://github.com/nfdi4plants',\n",
    "\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'github_orgs.json'                        # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for names in response.css('div.flex-auto')[7:]:\n",
    "            yield {\n",
    "                'repo-name': names.css('a.d-inline-block::text').get(),\n",
    "#                 'author': quote.css('span small::text').extract_first(),\n",
    "#                 'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "runner = CrawlerRunner({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "\n",
    "d = runner.crawl(QuotesSpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run() # the script will block here until the crawling is finished\n",
    "reactor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll github_orgs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy shell https://github.com/nfdi4plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"names\"\n",
    "    start_urls = [\n",
    "        'https://github.com/nfdi4plants',\n",
    "\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'github_orgs.json'                        # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for names in response.css('div.flex-auto')[7:]:\n",
    "            yield {\n",
    "                'repo-name': names.css('a.d-inline-block::text').get(),\n",
    "#                 'author': quote.css('span small::text').extract_first(),\n",
    "#                 'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.utils.log:Scrapy 2.4.1 started (bot: scrapybot)\n",
      "INFO:scrapy.utils.log:Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.5 (default, Nov  7 2019, 10:50:52) - [GCC 8.3.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1  11 Sep 2018), cryptography 2.1.4, Platform Linux-4.4.0-19041-Microsoft-x86_64-with-Ubuntu-18.04-bionic\n",
      "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "INFO:scrapy.crawler:Overridden settings:\n",
      "{'CLOSESPIDER_PAGECOUNT': 10,\n",
      " 'DOWNLOAD_TIMEOUT': 30,\n",
      " 'RETRY_TIMES': 1,\n",
      " 'TELNETCONSOLE_ENABLED': False}\n",
      "INFO:scrapy.middleware:Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.closespider.CloseSpider',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "INFO:scrapy.middleware:Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "INFO:scrapy.middleware:Enabled item pipelines:\n",
      "[]\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "DEBUG:scrapy.core.engine:Crawled (200) <GET https://github.com/nfdi4plants> (referer: None)\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            Swate\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            AnnotationPrinciples\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            nfdi4plants_ontology\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            ISADotNet\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            WebHub\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            Spawn\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            Branding\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            arcCommander\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            Swobup\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            ARC\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            arc-playground\\n'}\n",
      "DEBUG:scrapy.core.scraper:Scraped from <200 https://github.com/nfdi4plants>\n",
      "{'repo-name': '\\n            nfdi4plants.github.io\\n'}\n",
      "INFO:scrapy.core.engine:Closing spider (finished)\n",
      "INFO:scrapy.extensions.feedexport:Stored json feed (12 items) in: github_orgs.json\n",
      "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 225,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 20612,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.911843,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 2, 26, 21, 55, 36, 288047),\n",
      " 'item_scraped_count': 12,\n",
      " 'log_count/DEBUG': 13,\n",
      " 'log_count/INFO': 9,\n",
      " 'memusage/max': 125550592,\n",
      " 'memusage/startup': 125550592,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 2, 26, 21, 55, 35, 376204)}\n",
      "INFO:scrapy.core.engine:Spider closed (finished)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'repo-name': '\\n            Swate\\n'},\n",
       " {'repo-name': '\\n            AnnotationPrinciples\\n'},\n",
       " {'repo-name': '\\n            nfdi4plants_ontology\\n'},\n",
       " {'repo-name': '\\n            ISADotNet\\n'},\n",
       " {'repo-name': '\\n            WebHub\\n'},\n",
       " {'repo-name': '\\n            Spawn\\n'},\n",
       " {'repo-name': '\\n            Branding\\n'},\n",
       " {'repo-name': '\\n            arcCommander\\n'},\n",
       " {'repo-name': '\\n            Swobup\\n'},\n",
       " {'repo-name': '\\n            ARC\\n'},\n",
       " {'repo-name': '\\n            arc-playground\\n'},\n",
       " {'repo-name': '\\n            nfdi4plants.github.io\\n'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import scrapydo\n",
    "from scrapy import Request\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "scrapydo.setup()\n",
    "\n",
    "scrapydo.default_settings.update({\n",
    "    'LOG_LEVEL': 'DEBUG',\n",
    "    'CLOSESPIDER_PAGECOUNT': 10,\n",
    "    'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "    'FEED_URI': 'github_orgs.json' \n",
    "})\n",
    "\n",
    "lx = LinkExtractor()\n",
    "\n",
    "def parse_page(response):\n",
    "#     print(\"Got page: %s\" % response.url)\n",
    "    for names in response.css('div.flex-auto')[-12:]:\n",
    "        yield {\n",
    "                'repo-name': names.css('a.d-inline-block::text').extract_first(),}\n",
    "#     yield {'title': response.css('title::text').extract_first()}\n",
    "#     for link in lx.extract_links(response):\n",
    "#         yield Request(link.url, callback=parse_page)\n",
    "\n",
    "scrapydo.crawl(\"https://github.com/nfdi4plants\", callback=parse_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "Food to make:\n",
    "\n",
    "1. Cake\n",
    "    a. eggs\n",
    "    b. flour\n",
    "    c. milk\n",
    "    d. etc\n",
    "2. Salad\n",
    "    a. lettuce\n",
    "    b. spinach\n",
    "    c. cheese\n",
    "    d. ham\n",
    "    e. etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Food to make:',\n",
       " '',\n",
       " '1. Cake',\n",
       " '    a. eggs',\n",
       " '    b. flour',\n",
       " '    c. milk',\n",
       " '    d. etc',\n",
       " '2. Salad',\n",
       " '    a. lettuce',\n",
       " '    b. spinach',\n",
       " '    c. cheese',\n",
       " '    d. ham',\n",
       " '    e. etc',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,text in enumerate(a.split('\\n'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
